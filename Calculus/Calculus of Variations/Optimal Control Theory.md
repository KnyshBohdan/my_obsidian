Optimal Control Theory is a specialized branch of control theory that focuses on finding the most effective control for a dynamical system over a period of time to optimize a given objective function. This theory has a wide range of applications, from engineering systems like spacecraft to economic models.

#### Definition

Optimal Control Theory deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved. The control problem includes a cost functional, which is a function of state and control variables. An optimal control is a set of differential equations describing the paths of the control variables that minimize the cost function. Mathematically, the optimal control can be derived using Pontryagin's maximum principle or by solving the Hamilton–Jacobi–Bellman equations.

#### Examples

1. **Spacecraft Thrusters**: Finding the optimal control to reach the Moon with minimum fuel expenditure.
2. **Economic Models**: Minimizing unemployment through optimal fiscal and monetary policies.

#### Applications

- **Science**: Used in physics for system dynamics.
- **Engineering**: Applied in robotics, aerospace, and other engineering fields.
- **Operations Research**: Used for decision-making in complex systems.

#### Connection to Other Topics

- **[[Calculus of Variations]]**: Optimal control is an extension of the calculus of variations.
- **[[Hamilton's Principle]]**: The Hamilton–Jacobi–Bellman equation in optimal control is related to Hamilton's principle.
- **[[Dynamical Systems]]**: The theory is fundamentally concerned with the behavior of dynamical systems.
- **[[Numerical Methods]]**: Numerical methods are often used to solve optimal control problems that don't have analytical solutions.

[[Calculus]]
[[Calculus of Variations]]