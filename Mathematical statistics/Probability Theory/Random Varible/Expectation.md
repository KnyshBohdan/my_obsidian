In probability theory, the expected value (also known as expectation, expectancy, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. It represents the arithmetic mean of a large number of independently selected outcomes of a random variable.

## Definition

### Random Variables with Finitely Many Outcomes

For a random variable with a finite list of possible outcomes, the expectation is defined as:

$$E[X]=x_1p_1+x_2p_2+⋯+x_kp_k$$

### Random Variables with Countably Many Outcomes

For a random variable with a countable set of possible outcomes, the expectation is defined as:

$$E[X] = \sum_{i=1}^{\infty} x_i p_i$$

provided that the infinite sum converges absolutely.

## Examples

### Roll of a Fair Six-Sided Die

Let X represent the outcome of a roll of a fair six-sided die. The expectation of XX is:

$$E[X] = 1/6 + 2/6 + 3/6 + ... + 6/6 = 3.5 $$

### Roulette Game

In American roulette, the expected profit from a $1 bet on a single number is −119​, considering the probability of winning and losing.

## Historical Background

The idea of the expected value originated in the 17th century from the study of the problem of points. Blaise Pascal and Pierre de Fermat independently came up with a solution, laying down the foundations of the theory of probability.

## Connections to Other Topics

- [[Random Variable]]: Expectation is a key property of a random variable, describing the average value.
- [[Probability Theory]]: Expectation is a core concept within Probability Theory, providing a mathematical framework for describing average outcomes.
- [[Mathematical statistics]]: Expectation plays a significant role in Mathematical Statistics, especially in statistical modeling and analysis.